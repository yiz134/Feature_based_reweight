{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "701334e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.core.frozen_dict import FrozenDict\n",
    "from flax.training.train_state import TrainState\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d06017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projection import *\n",
    "from data import *\n",
    "from resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lr_schedule(m: int, batch_size: int, num_epochs: int, base_lr: float):\n",
    "    steps_per_epoch = math.ceil(m / batch_size)\n",
    "    total_steps = num_epochs * steps_per_epoch\n",
    "\n",
    "    # warmup for first 5 epochs\n",
    "    warmup_steps = 5 * steps_per_epoch\n",
    "    warmup = optax.linear_schedule(\n",
    "        init_value=0.0, end_value=base_lr, transition_steps=warmup_steps\n",
    "    )\n",
    "\n",
    "    # piecewise after epoch 150 and 250\n",
    "    boundary_150 = 150 * steps_per_epoch\n",
    "    boundary_250 = 250 * steps_per_epoch\n",
    "    main = optax.piecewise_constant_schedule(\n",
    "        init_value=base_lr,\n",
    "        boundaries_and_scales={\n",
    "            boundary_150: 0.1,\n",
    "            boundary_250: 0.1,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    lr_schedule = optax.join_schedules(\n",
    "        schedules=[warmup, main],\n",
    "        boundaries=[warmup_steps],\n",
    "    )\n",
    "    return lr_schedule\n",
    "\n",
    "def make_decay_mask(params):\n",
    "    \"\"\"\n",
    "    Recursively traverse `params` (which is a FrozenDict). For every leaf\n",
    "    (i.e. array), return True if its key-name is \"kernel\", else False.\n",
    "    Return a new FrozenDict of booleans with identical tree structure.\n",
    "    \"\"\"\n",
    "    def recurse(tree):\n",
    "        if isinstance(tree, FrozenDict):\n",
    "            # Recurse into each sub‐FrozenDict\n",
    "            return FrozenDict({k: recurse(v) for k, v in tree.items()})\n",
    "        else:\n",
    "            # At a leaf: tree is an ndarray. The caller’s key told us whether\n",
    "            # this leaf’s name was \"kernel\"—but here we only reach the leaf value,\n",
    "            # so instead we rely on the fact that in recurse’s caller we know the key.\n",
    "            # To work around that, we’ll rewrite this function to accept (tree, key).\n",
    "            raise RuntimeError(\"Should not hit leaf without key context\")\n",
    "    # Instead, define a helper that carries the key in recursion:\n",
    "    def recurse_with_key(tree, key_name):\n",
    "        if isinstance(tree, FrozenDict):\n",
    "            return FrozenDict({k: recurse_with_key(v, k) for k, v in tree.items()})\n",
    "        else:\n",
    "            # leaf array; use the most‐recent key_name\n",
    "            return (key_name == \"kernel\")\n",
    "    return recurse_with_key(params, None)\n",
    "\n",
    "\n",
    "def compute_pi_inv(y_tr: jnp.ndarray):\n",
    "    n = y_tr.shape[0]\n",
    "    counts = jnp.bincount(y_tr)\n",
    "    return n / counts[y_tr]  # shape (n,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d719dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, xb, yb, wb):\n",
    "    def loss_fn(params, batch_stats):\n",
    "        vars = {\"params\": params, \"batch_stats\": batch_stats}\n",
    "        (logits, new_vars) = state.apply_fn(vars, xb, train=True, mutable=[\"batch_stats\"])\n",
    "        y_oh = jax.nn.one_hot(yb, logits.shape[-1])\n",
    "        ce = -jnp.sum(y_oh * jax.nn.log_softmax(logits), axis=-1)\n",
    "        loss = jnp.mean(wb * ce)\n",
    "        return loss, new_vars[\"batch_stats\"]\n",
    "\n",
    "    (loss, new_bs), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n",
    "        state.params, state.batch_stats\n",
    "    )\n",
    "    state = state.apply_gradients(grads=grads, batch_stats=new_bs)\n",
    "    return state, loss\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state, xb, wb=None, yb=None):\n",
    "    vars = {\"params\": state.params, \"batch_stats\": state.batch_stats}\n",
    "    logits = state.apply_fn(vars, xb, train=False, mutable=False)\n",
    "    if (wb is not None) and (yb is not None):\n",
    "        y_oh = jax.nn.one_hot(yb, logits.shape[-1])\n",
    "        ce = -jnp.sum(y_oh * jax.nn.log_softmax(logits), axis=-1)\n",
    "        loss = jnp.sum(wb * ce) / jnp.sum(wb)\n",
    "        return loss, logits\n",
    "    return logits\n",
    "\n",
    "@jax.jit\n",
    "def compute_residuals(apply_fn, params, batch_stats, X, y):\n",
    "    vars = {\"params\": params, \"batch_stats\": batch_stats}\n",
    "    logits = apply_fn(vars, X, train=False, mutable=False)\n",
    "    probs = jax.nn.softmax(logits)\n",
    "    idx = jnp.arange(y.shape[0])\n",
    "    return probs[idx, y] - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "273d375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_in_batches(state, X_all, y_all=None, w_all=None, batch_size=512):\n",
    "    N = X_all.shape[0]\n",
    "    num_steps = int(jnp.ceil(N / batch_size))\n",
    "\n",
    "    all_logits = []\n",
    "    total_num = 0.0\n",
    "    total_den = 0.0\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, N)\n",
    "        xb = X_all[start:end]\n",
    "\n",
    "        if (w_all is None) or (y_all is None):\n",
    "            logits = eval_step(state, xb)\n",
    "            all_logits.append(logits)\n",
    "        else:\n",
    "            wb = w_all[start:end]\n",
    "            yb = y_all[start:end]\n",
    "            loss_chunk, logits = eval_step(state, xb, wb, yb)\n",
    "            all_logits.append(logits)\n",
    "            sum_wb = float(jnp.sum(wb))\n",
    "            total_den += sum_wb\n",
    "            total_num += float(loss_chunk) * sum_wb\n",
    "\n",
    "    all_logits = jnp.concatenate(all_logits, axis=0)\n",
    "    if (w_all is None) or (y_all is None):\n",
    "        return all_logits\n",
    "    avg_loss = total_num / total_den\n",
    "    return avg_loss, all_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce502b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_step(\n",
    "    state,\n",
    "    X_tr, y_tr, w, X_val, y_val,\n",
    "    pi_inv, sign_mask, eta, noise_limit, proj,\n",
    "):\n",
    "    params = state.params\n",
    "    bs = state.batch_stats\n",
    "\n",
    "    # empirical_ntk_vp_fn returns a callable f: R^{N_val×C} → R^{N_tr×C'}\n",
    "    f_all = lambda X_in: state.apply_fn(\n",
    "        {\"params\": params, \"batch_stats\": bs},\n",
    "        X_in, train=False, mutable=False\n",
    "    )\n",
    "    ntk_vp_all = nt.empirical_ntk_vp_fn(f_all, X_tr, X_val, params)\n",
    "\n",
    "    # compute factor = n_tr / sum_i [ π_i * sum_j sign_mask[i,j] * full_ones[j,i] ]\n",
    "    C = f_all(X_tr[:1]).shape[-1]\n",
    "    ones_val = jnp.ones((X_val.shape[0], C))\n",
    "    full_ones = ntk_vp_all(ones_val)  # shape (n_tr, )\n",
    "    sum_entries = jnp.sum(pi_inv * jnp.sum(sign_mask * full_ones, axis=1))\n",
    "    factor = X_tr.shape[0] / sum_entries\n",
    "\n",
    "    def signed_vp(u_val):\n",
    "        # u_val: (n_val,)\n",
    "        u_mat = jnp.tile(u_val[:, None], (1, C))\n",
    "        full = ntk_vp_all(u_mat)  # shape (n_tr, C)\n",
    "        weighted = pi_inv * jnp.sum(sign_mask * full, axis=1)\n",
    "        return factor * weighted  # shape (n_tr,)\n",
    "    \n",
    "    @jax.jit\n",
    "    def compute_delta_w(u_train, u_val):\n",
    "       signed = signed_vp(u_val)  # returns (n_tr,)\n",
    "       raw = u_train * signed\n",
    "       centered = raw - jnp.mean(raw)\n",
    "       clipped = jnp.minimum(centered, 0.0)\n",
    "       return clipped\n",
    "\n",
    "    u_train = compute_residuals(state.apply_fn, params, bs, X_tr, y_tr)\n",
    "    u_val = compute_residuals(state.apply_fn, params, bs, X_val, y_val)\n",
    "    delta_w = compute_delta_w(u_train, u_val)\n",
    "    w_new = w - eta * delta_w\n",
    "\n",
    "    # project onto top (n_tr - noise_rate * n_tr) coordinates\n",
    "    if proj == \"euclidean\":\n",
    "        w_proj, _ = project_euclidean(w_new, int(X_tr.shape[0] - noise_limit))\n",
    "    else:  # 'lp'\n",
    "        w_proj = project_lp(w_new, int(X_tr.shape[0] - noise_limit))\n",
    "\n",
    "    return w_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a075e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    rng,\n",
    "    X_tr, y_tr, X_val, y_val, X_te, y_te,\n",
    "    alpha0=0.5,\n",
    "    eta=0.1,\n",
    "    lr=0.1,\n",
    "    batch_size=128,\n",
    "    num_epochs=350,\n",
    "    reweight_every=5,\n",
    "    noise_rate=0.4,\n",
    "    proj=\"lp\",\n",
    "):\n",
    "    m = X_tr.shape[0]\n",
    "\n",
    "    # 5.1) Initialize model, params, batch_stats, optimizer\n",
    "    rng, init_key = jax.random.split(rng)\n",
    "    model = ResNet18(num_classes=10)\n",
    "    dummy = jnp.zeros((1, *X_tr.shape[1:]), dtype=jnp.float32)\n",
    "    variables = model.init(init_key, dummy, train=True)\n",
    "    params, batch_stats = variables[\"params\"], variables[\"batch_stats\"]\n",
    "\n",
    "    lr_schedule = make_lr_schedule(m, batch_size, num_epochs, lr)\n",
    "    decay_mask = make_decay_mask(params)  # <-- mirror params exactly\n",
    "    tx = optax.chain(\n",
    "        optax.add_decayed_weights(5e-4, mask=decay_mask),\n",
    "        optax.sgd(learning_rate=lr_schedule, momentum=0.9),\n",
    "    )\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        batch_stats=batch_stats,\n",
    "        tx=tx,\n",
    "    )\n",
    "\n",
    "    # 5.2) Precompute class reweight constants\n",
    "    pi_inv = compute_pi_inv(y_tr)                  # shape (m,)\n",
    "    C = jnp.max(y_tr) + 1                           # num_classes\n",
    "    class_ids = jnp.arange(C)\n",
    "    # sign_mask: shape (m, C), +1 where y_tr matches class, −alpha0 otherwise\n",
    "    sign_mask = jnp.where(y_tr[:, None] == class_ids[None, :], 1.0, -alpha0)\n",
    "\n",
    "    w = jnp.ones((m,))     # initial example weights\n",
    "    w_history = [w]\n",
    "\n",
    "    stats = {\"tr_loss\": [], \"val_loss\": [], \"tr_acc\": [], \"te_acc\": []}\n",
    "\n",
    "    steps_per_epoch = math.ceil(m / batch_size)\n",
    "\n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        # — Shuffle & minibatches\n",
    "        rng, sk = jax.random.split(rng)\n",
    "        perm = jax.random.permutation(sk, m)\n",
    "        Xs, ys, ws = X_tr[perm], y_tr[perm], w[perm]\n",
    "\n",
    "        num_batches = int(jnp.ceil(m / batch_size))\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min((i + 1) * batch_size, m)\n",
    "            xb, yb, wb = Xs[start:end], ys[start:end], ws[start:end]\n",
    "\n",
    "            rng, subkey = jax.random.split(rng)\n",
    "            xb_aug, rng = augment_batch(subkey, xb)\n",
    "\n",
    "            state, _ = train_step(state, xb_aug, yb, wb)\n",
    "\n",
    "        # — Reweight every few epochs (after warmup)\n",
    "        if (epoch > 5) and (epoch % reweight_every == 0):\n",
    "            w = reweight_step(\n",
    "                state,\n",
    "                X_tr, y_tr, w,\n",
    "                X_val, y_val,\n",
    "                pi_inv, sign_mask,\n",
    "                eta, noise_rate * m, proj,\n",
    "            )\n",
    "\n",
    "        # — Logging every 10 epochs (or first)\n",
    "        if (epoch == 1) or (epoch % 10 == 0):\n",
    "            w_history.append(w)\n",
    "            loss_tr, logits_tr = evaluate_in_batches(state, X_tr, y_tr, w, batch_size=512)\n",
    "            loss_val, _ = evaluate_in_batches(state, X_val, y_val, jnp.ones_like(y_val), batch_size=512)\n",
    "            logits_te = evaluate_in_batches(state, X_te, y_all=None, w_all=None, batch_size=512)\n",
    "\n",
    "            tr_acc = jnp.mean(jnp.argmax(logits_tr, -1) == y_tr).item()\n",
    "            te_acc = jnp.mean(jnp.argmax(logits_te, -1) == y_te).item()\n",
    "\n",
    "            stats[\"tr_loss\"].append(float(loss_tr))\n",
    "            stats[\"val_loss\"].append(float(loss_val))\n",
    "            stats[\"tr_acc\"].append(tr_acc)\n",
    "            stats[\"te_acc\"].append(te_acc)\n",
    "\n",
    "            if (len(stats[\"val_loss\"]) > 1) and (stats[\"val_loss\"][-1] > stats[\"val_loss\"][-2]):\n",
    "                print(f\"Epoch {epoch:02d}: val_loss increased; consider early stopping.\")\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch:02d} | \"\n",
    "                f\"tr_loss={loss_tr:.4f}, val_loss={loss_val:.4f} | \"\n",
    "                f\"tr_acc={tr_acc*100:.2f}%, te_acc={te_acc*100:.2f}% | \"\n",
    "                f\"||Δw||={jnp.linalg.norm(w_history[-1] - w_history[-2]):.4f}\"\n",
    "            )\n",
    "\n",
    "    return state.params, w, w_history, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59066355",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.PRNGKey(0)\n",
    "X_tr, y_tr, y_tr_noisy, X_val, y_val, X_te, y_te = create_noisy_cifar(rng, subsample=(40000, 10000), noise_rate=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5918b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5510a5017e914783ac324d2cfb9c103f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | tr_loss=2.2702, val_loss=2.1409 | tr_acc=17.21%, te_acc=22.12% | ||Δw||=0.0000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params_final, w_final, w_list, stats0 = train_loop(\n",
    "    rng, X_tr, y_tr_noisy, X_val, y_val, X_te, y_te,\n",
    "    alpha0=0.5, eta=1.0, lr=0.1,\n",
    "    batch_size=128, num_epochs=350, reweight_every=np.inf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c203c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bilevel_reweight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
